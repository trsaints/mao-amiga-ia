{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fa03ed69",
   "metadata": {},
   "source": [
    "# Processamento da Fonte de Dados\n",
    "\n",
    "Este notebook realiza o processo de ETL (extract, transform, load) para as fontes de dados disponibilizadas pelo [Mapa das Organizações da Sociedade Civil](https://mapaosc.ipea.gov.br/base-dados).\n",
    "\n",
    "## Configurando o Ambiente\n",
    "\n",
    "Para manutenção da simplicidade, os códigos necessários para realizar o processamento dos dados estão segmentados em módulos. Este caderno tem o propósito apenas de orquestrar e exibir os resultados dos processamentos realizados por tais módulos.\n",
    "\n",
    "Para isto, é necessário reconfigurar o ambiente do notebook, para que os caminhos dos módulos sejam resolvidos corretamente, como em uma aplicação convencional Python.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efff15bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "project_root = os.path.abspath(os.path.join(os.getcwd(), '..'))\n",
    "\n",
    "if project_root not in sys.path:\n",
    "    sys.path.insert(0, project_root)\n",
    "\n",
    "print(f\"Added to Python path: {project_root}\")\n",
    "print(f\"Current working directory: {os.getcwd()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "005ac392",
   "metadata": {},
   "source": [
    "## Etapas do Processamento\n",
    "\n",
    "Este caderno executa as seguintes etapas para produzir uma base de dados tratada. Assume-se que as fontes de dados brutas estão disponíveis no diretório `datasets` deste repositório.\n",
    "\n",
    "### 1. Conversão de Encoding\n",
    "\n",
    "Os datasets disponibilizados encontram-se em diversos formatos, além de contarem com \"encodings\" inadequados para a leitura e análise adequada através de algoritmos.\n",
    "\n",
    "Para cada dataset original, esta etapa gera fontes contendo encoding adequado (em utf8) para a realização de etapas posteriores.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31d1a803",
   "metadata": {},
   "outputs": [],
   "source": [
    "from data.processing.data_parser import to_utf8\n",
    "\n",
    "ONGS_DATASET = \"../datasets/osc_2025_2.csv\"\n",
    "PROJECTS_DATASET = \"../datasets/projetos.csv\"\n",
    "\n",
    "parsed_ongs_source = to_utf8(ONGS_DATASET,\n",
    "                             separator=\";\",\n",
    "                             input_encoding=\"latin1\")\n",
    "parsed_projects_source = to_utf8(PROJECTS_DATASET)\n",
    "\n",
    "if parsed_ongs_source is None:\n",
    "    raise FileNotFoundError(\n",
    "        f\"File {ONGS_DATASET} not found or could not be converted to UTF-8.\")\n",
    "\n",
    "if parsed_projects_source is None:\n",
    "    raise FileNotFoundError(\n",
    "        f\"File {PROJECTS_DATASET} not found or could not be converted to UTF-8.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5faf941a",
   "metadata": {},
   "source": [
    "### 2. Tratamento de Organizações\n",
    "\n",
    "A segunda fase lida com as informações disponibilizadas na base `osc_2025_2.csv`, que contém informações sobre as OSCs catalogadas na fonte mencionada.\n",
    "\n",
    "#### 2.1. Tratameno de Identificadores\n",
    "\n",
    "Para garantir que possíveis referências cruzadas sejam executadas corretamente, o CNPJ, originalmente vindo somente como dígitos (e sem zeros à esquerda), é formatado\n",
    "para o formato convencional `XX.XXX.XXX/XXXX-XX`:\n",
    "\n",
    "| Cnpj (Fonte)   | Cnpj (Formatado)   |\n",
    "| -------------- | ------------------ |\n",
    "| 00000000000    | 00.000.000/0000-00 |\n",
    "| 11111111111111 | ...                |\n",
    "\n",
    "#### 2.2. Tratamento de Áreas de Atuação\n",
    "\n",
    "O dataset original armazena os códigos de área de atuação de cada OSC de maneira tabular, conforme o exemplo abaixo:\n",
    "\n",
    "| Cnpj               | ... | Area_x | Area_y | Area_z |\n",
    "| ------------------ | --- | ------ | ------ | ------ |\n",
    "| 00.000.000/0000-00 | ... | 0      | 1      | 0      |\n",
    "| 11.111.111/1111-11 | ... | 1      | 0      | 1      |\n",
    "\n",
    "Onde valores `1` indicam que a OSC faz parte da área de atuação correspondente à coluna.\n",
    "\n",
    "Para melhorar a legibilidade do dataset final, esta estrutura é trauzida e concatenada em um único texto descritivo, conforme o exemplo abaixo:\n",
    "\n",
    "| Cnpj               | ... | Areas de Atuação |\n",
    "| ------------------ | --- | ---------------- |\n",
    "| 00.000.000/0000-00 | ... | Area y           |\n",
    "| 11.111.111/1111-11 | ... | Area x, Area z   |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f41e9e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from data.processing.data_parser import write_dataset\n",
    "from data.processing.osc import osc_dataset\n",
    "from pandas import read_csv\n",
    "\n",
    "osc_source = read_csv(parsed_ongs_source, sep=\";\", dtype=str)\n",
    "osc_df = osc_dataset(osc_source)\n",
    "osc_path = write_dataset(\"osc\", osc_df)\n",
    "\n",
    "if (osc_path is None) or (not os.path.isfile(osc_path)):\n",
    "    raise FileNotFoundError(\n",
    "        f\"File {osc_path} not found or could not be created.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00acc761",
   "metadata": {},
   "outputs": [],
   "source": [
    "osc_df.head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c07a0cf1",
   "metadata": {},
   "source": [
    "### 3. Tratamento de Projetos\n",
    "\n",
    "Projetos representam as ações sociais realizadas pelas OSCs.\n",
    "Para garantir sanitização adequada de informações,\n",
    "o dataset final gerado pelo ETL aplica os seguintes critérios de filtragem:\n",
    "\n",
    "- Somente projetos que forneçam data de início;\n",
    "- Somente projetos que contenham ao menos uma das seguintes informações:\n",
    "  - O total de beneficiados;\n",
    "  - O valor captado;\n",
    "  - O valor total;\n",
    "- Somente projetos do Distrito Federal\n",
    "\n",
    "> Além disso, duplicatas serão removidas do dataset final. Para este dataset, considera-se uma duplicata projetos que contenham o mesmo valor para a coluna \"Id Projeto\".\n",
    "\n",
    "#### 3.1. Filtrando Projetos por Região\n",
    "\n",
    "Internamente, o método `projects_dataset` aplica um filtro para retornar somente projetos cuja região esteja no Distrito Federal. Para isso, o método `projects.by_region` é utilizado para realizar o cruzamento de referência entre as colunas \"CNPJ OSC\", do dataset de projetos, e \"CNPJ\", do dataset de OSCs.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be64b6fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from data.processing.data_parser import write_dataset\n",
    "from data.processing.projects import projects_dataset\n",
    "from pandas import read_csv\n",
    "\n",
    "projects_source = read_csv(parsed_projects_source, sep=\";\", dtype=str)\n",
    "projects_df = projects_dataset(projects_source, osc_df)\n",
    "projects_path = write_dataset(\"projects\", projects_df)\n",
    "\n",
    "if (projects_path is None) or (not os.path.isfile(projects_path)):\n",
    "    raise FileNotFoundError(\n",
    "        f\"File {projects_path} not found or could not be created.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21528432",
   "metadata": {},
   "outputs": [],
   "source": [
    "projects_df.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "096cf279",
   "metadata": {},
   "outputs": [],
   "source": [
    "from model.decision_tree_model import DecisionTreeModel\n",
    "from model.naive_bayes_model import NaiveBayesModel\n",
    "from model.svm_model import SVMModel\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import pandas as pd\n",
    "\n",
    "# Carregar o dataset tratado\n",
    "dataset_path = projects_path\n",
    "data = pd.read_csv(dataset_path, sep=\";\", dtype=str)\n",
    "data.fillna('', inplace=True)\n",
    "\n",
    "# Codificar variáveis categóricas\n",
    "label_encoders = {}\n",
    "for column in data.select_dtypes(include=['object']).columns:\n",
    "    le = LabelEncoder()\n",
    "    data[column] = le.fit_transform(data[column])\n",
    "    label_encoders[column] = le\n",
    "\n",
    "# Separar features e target\n",
    "X = data.drop('Status', axis=1)\n",
    "y = data['Status']\n",
    "\n",
    "# Treinar e avaliar modelos\n",
    "models = {\n",
    "    'Decision Tree': DecisionTreeModel(),\n",
    "    'Naive Bayes': NaiveBayesModel(),\n",
    "    'SVM': SVMModel()\n",
    "}\n",
    "\n",
    "for model_name, model in models.items():\n",
    "    print(f\"\\nTraining and evaluating {model_name}...\")\n",
    "    \n",
    "    # Atribuir os dados ao modelo\n",
    "    model.X = X\n",
    "    model.y = y\n",
    "    \n",
    "    model.train()\n",
    "    model.evaluate() "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.12.9)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
